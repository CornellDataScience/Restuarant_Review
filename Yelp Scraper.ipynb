{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 length of reviews\n",
      "['10/21/2018', '10/20/2018', '9/1/2018', '9/4/2017', '7/7/2018', '11/13/2018', '9/22/2017', '10/20/2018', '9/9/2018', '11/18/2017', '8/15/2018', '6/26/2018', '3/8/2018', '7/12/2018', '12/15/2017', '4/30/2018', '3/27/2018', '9/12/2018', '10/12/2018', '8/10/2017', '8/11/2017', '10/15/2017', '4/13/2018', '8/24/2017', '4/15/2018', '8/5/2018', '7/15/2017', '3/11/2017', '3/26/2016', '8/7/2018', '3/13/2017', '11/6/2017', '2/19/2017', '3/12/2017', '11/25/2017', '2/18/2018', '12/29/2016', '8/15/2017', '9/23/2018', '9/23/2017', '12/16/2017', '12/4/2017', '9/14/2017', '9/8/2016', '1/7/2018', '2/7/2018', '2/18/2016', '4/23/2017', '10/13/2016', '4/19/2017', '7/23/2017', '8/12/2017', '5/28/2017', '2/11/2017', '3/25/2016', '4/16/2016', '3/22/2016', '2/27/2016', '5/29/2016', '6/2/2017', '5/5/2017', '6/3/2016', '6/22/2016']\n",
      "63 length of dates\n",
      "['The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook', 'The Rook']\n",
      "[5, 5, 4, 5, 5, 2, 5, 5, 5, 4, 5, 4, 5, 5, 4, 5, 5, 5, 5, 2, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 4, 3, 5, 5, 3, 5, 5, 3, 3, 4, 2, 3, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 2, 5, 5, 5, 5, 5, 5, 5, 3, 5]\n",
      "ALL PAGE NUMBERS: \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4]\n",
      "[1, 0, 0, 0, 1, 0, 10, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 3, 5, 0, 2, 0, 2, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 3, 0, 2, 0, 0, 0, 1, 0, 4, 2, 4, 5, 1, 0, 0, 0, 3]\n",
      "[1, 0, 0, 0, 1, 0, 10, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 3, 5, 0, 2, 0, 2, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 3, 0, 2, 0, 0, 0, 1, 0, 4, 2, 4, 5, 1, 0, 0, 0, 3]\n",
      "[1, 0, 0, 0, 1, 0, 10, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 3, 5, 0, 2, 0, 2, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 3, 0, 2, 0, 0, 0, 1, 0, 4, 2, 4, 5, 1, 0, 0, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from lxml.html import fromstring\n",
    "\n",
    "def getProxies(url):\n",
    "    req = requests.get('https://free-proxy-list.net/')\n",
    "    soup = BeautifulSoup(req.content,'lxml')\n",
    "    rows = soup.find_all('tr')\n",
    "    proxyList = []\n",
    "    sp = 0\n",
    "    for row in rows:\n",
    "        if(row.findChild('td',class_='hx',recursive = False) != None):\n",
    "            if(row.findChild('td',class_='hx',recursive = False).text == 'yes'):\n",
    "                proxyList.append(row.td.text)\n",
    "                proxyID = row.td.text\n",
    "                try:\n",
    "                    proxy = {\n",
    "                        \"http\": 'http://1.4.221.231',\n",
    "                        \"https\": 'http://' + proxyID       \n",
    "                    }\n",
    "                    r = requests.get('https://www.google.com/',proxies=proxy) \n",
    "                    sp = BeautifulSoup(r.content,'lxml')\n",
    "                    System.out.println('This proxy worked: ' + proxyID)\n",
    "                    return sp\n",
    "                except:\n",
    "                    print(row.td.text + ' Proxy didnt work')\n",
    "def scrapeYelp(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content,'lxml')\n",
    "    page_Divs = soup.find_all('div',class_='arrange_unit page-option')\n",
    "    page_links = []\n",
    "    page_links.append(url)\n",
    "    for div in page_Divs:\n",
    "        page_links.append(div.findChild('a')['href'])\n",
    "    name_div = soup.find('div',class_='biz-page-header clearfix')\n",
    "    end_div = name_div.findChild('div',class_='u-inline-block')\n",
    "    currentRestaurant = name_div.findChild('h1',recursive = True).text + ' ' +end_div.findChild('h1').text\n",
    "    allReviews = []\n",
    "    allDates = []\n",
    "    allRatings = []\n",
    "    allRestaurants = [] \n",
    "    allPageNumbers = []\n",
    "    allUsefulVotes = []\n",
    "    allFunnyVotes = []\n",
    "    allCoolVotes = []\n",
    "    for i,link in enumerate(page_links):\n",
    "        r = requests.get(link)\n",
    "        soup = BeautifulSoup(r.content,'lxml')             \n",
    "        for review in soup.find_all('div',class_='review-content'):\n",
    "            allPageNumbers.append(i + 1)\n",
    "            allRestaurants.append(currentRestaurant)\n",
    "            reviewText = review.findChild('p',recursive=False).text\n",
    "            allReviews.append(reviewText)\n",
    "            dateText = review.findChild('span', recursive = True).text\n",
    "            dateText = dateText.replace('\\n','')\n",
    "            dateText = dateText.replace('Updated review','')\n",
    "            dateText = dateText.strip()\n",
    "            allDates.append(dateText)\n",
    "            ratingDiv = review.findChild('div',class_='biz-rating biz-rating-large clearfix')\n",
    "            rating = ratingDiv.findChild('div',recursive=False).findChild('div',recursive=False)\n",
    "            if(rating['title'] == '5.0 star rating'):\n",
    "                allRatings.append(5)\n",
    "            elif(rating['title'] == '4.0 star rating'):\n",
    "                allRatings.append(4)\n",
    "            elif(rating['title'] == '3.0 star rating'):\n",
    "                allRatings.append(3)\n",
    "            elif(rating['title'] == '2.0 star rating'):\n",
    "                allRatings.append(2)\n",
    "            else:\n",
    "                allRatings.append(1)\n",
    "            voteCount = 0\n",
    "        for voteDiv in soup.find_all('div',class_='rateReview voting-feedback'):\n",
    "            voteStr = voteDiv.findChildren('span',class_='vote-type',recursive=True)\n",
    "            for vote in voteStr:\n",
    "                if(vote.text == 'Useful'):\n",
    "                    voteCount = voteDiv.findChild('span',class_='count',recursive=True).text\n",
    "                    if(voteCount == ''):\n",
    "                        allUsefulVotes.append(0)\n",
    "                    else:\n",
    "                        allUsefulVotes.append(int(voteCount))\n",
    "                elif(vote.text == 'Funny'):\n",
    "                    voteCount = voteDiv.findChild('span',class_='count',recursive=True).text\n",
    "                    if(voteCount == ''):\n",
    "                        allFunnyVotes.append(0)\n",
    "                    else:\n",
    "                        allFunnyVotes.append(int(voteCount))\n",
    "                elif(vote.text == 'Cool'):\n",
    "                    voteCount = voteDiv.findChild('span',class_='count',recursive=True).text\n",
    "                    if(voteCount == ''):\n",
    "                        allCoolVotes.append(0)\n",
    "                    else:\n",
    "                        allCoolVotes.append(int(voteCount))\n",
    "            \n",
    "    print(str(len(allReviews)) + ' length of reviews')\n",
    "    print(allDates)\n",
    "    print(str(len(allDates)) + ' length of dates')\n",
    "    print(allRestaurants)\n",
    "    print(allRatings)\n",
    "    print('ALL PAGE NUMBERS: ')\n",
    "    print(allPageNumbers)\n",
    "    print((allUsefulVotes))\n",
    "    print(allFunnyVotes)\n",
    "    print(allCoolVotes)       \n",
    "scrapeYelp('https://www.yelp.com/biz/the-rook-ithaca')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
